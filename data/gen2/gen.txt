
library("gbm")
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/gen2/TRAINING_2.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/gen2/TEST_2.txt", sep="\t", header=TRUE)

response_column <- which(colnames(traindf) == "RESP")
trainy <- traindf$RESP

numOfTrees <- 500
gbm_formula <- as.formula(paste0("RESP ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))


system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = numOfTrees, bag.fraction = 1, interaction.depth = 3))

summary.gbm(gbm_model)

predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = numOfTrees, type = "response")
sqrt(mean((testdf$RESP - predictions_gbm)^2))

predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = numOfTrees, type = "response")
sqrt(mean((traindf$RESP - predictions_train_gbm)^2))

pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = numOfTrees)

ideas - pick a subset of attributes to best pslit on instead of all. Did blum give me this idea?